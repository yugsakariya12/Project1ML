import requests
import socket
import tldextract
import re
import whois
import math
from urllib.parse import urlparse
from datetime import datetime
from collections import Counter
import warnings

warnings.filterwarnings('ignore')


# ==========================================================
# URL FEATURE EXTRACTION
# ==========================================================
def fetch_url_data(url):
    data = {'url': url}

    try:
        parsed = urlparse(url)
        domain = parsed.netloc

        data['url_length'] = len(url)
        data['https_enabled'] = parsed.scheme == 'https'
        data['domain'] = domain

        # IP Address
        try:
            data['ip_address'] = socket.gethostbyname(domain)
        except:
            data['ip_address'] = "Unknown"

        # TLD + Domain Info
        ext = tldextract.extract(url)
        data['top_level_domain'] = ext.suffix
        data['domain_name'] = ext.domain
        data['subdomain'] = ext.subdomain

        # WHOIS
        try:
            w = whois.whois(domain)
            data['whois_complete'] = bool(w.domain_name)
            data['creation_date'] = str(w.creation_date) if w.creation_date else "Unknown"
            data['expiration_date'] = str(w.expiration_date) if w.expiration_date else "Unknown"
            data['registrar'] = w.registrar if w.registrar else "Unknown"
        except:
            data['whois_complete'] = False
            data['creation_date'] = "Failed"
            data['expiration_date'] = "Failed"
            data['registrar'] = "Unknown"

        # Fetch Web Page
        headers = {'User-Agent': 'Mozilla/5.0'}
        r = requests.get(url, headers=headers, timeout=10, verify=False)
        data['status_code'] = r.status_code
        data['page_size'] = len(r.content)
        data['content_type'] = r.headers.get('Content-Type', 'Unknown')
        data['server'] = r.headers.get('Server', 'Unknown')
        data['last_modified'] = r.headers.get('Last-Modified', 'Unknown')

        # Title
        title = re.search(r'<title>(.*?)</title>', r.text, re.I)
        data['page_title'] = title.group(1) if title else "No title"

        # JavaScript Analysis
        scripts = re.findall(r'<script.*?>.*?</script>', r.text, re.I | re.S)
        js = ''.join(scripts)
        data['javascript_size'] = len(js)

        obf_patterns = [
            r'eval\(', r'escape\(', r'String\.fromCharCode',
            r'\\x[0-9a-fA-F]{2}', r'%[0-9a-fA-F]{2}'
        ]
        obf_count = sum(len(re.findall(p, js)) for p in obf_patterns)
        data['javascript_obfuscation_ratio'] = round(obf_count / (len(js) + 1) * 100, 2)

        behavior_patterns = [
            'document.cookie','window.location','fetch(','xmlhttprequest'
        ]
        data['js_behavior_score'] = sum(js.lower().count(x) for x in behavior_patterns) * 3

        # Other Signals
        data['iframe_count'] = len(re.findall(r'<iframe', r.text, re.I))
        data['form_count'] = len(re.findall(r'<form', r.text, re.I))
        data['external_resources'] = len(re.findall(r'(src|href)=["\']http', r.text, re.I))

        # Suspicious URL patterns
        suspicious = [
            'login','verify','account','secure','update','billing',
            'signin','password','bank','paypal','amazon','apple'
        ]
        data['suspicious_url_pattern'] = any(x in url.lower() for x in suspicious)

        # URL Shorteners
        shorteners = ['bit.ly','tinyurl','t.co','goo.gl']
        data['url_shortener'] = any(x in domain for x in shorteners)

        # Risky TLDs
        risky_tlds = ['.tk','.ml','.ga','.cf','.gq','.xyz','.top']
        data['high_risk_tld'] = any(url.endswith(x) for x in risky_tlds)

        return data

    except Exception as e:
        data['error'] = str(e)
        return data


# ==========================================================
# ADVANCED ANALYSIS
# ==========================================================
def url_entropy(url):
    p = [n / len(url) for n in Counter(url).values()]
    return -sum(x * math.log2(x) for x in p)

def trust_score_adjustment(data):
    trusted = ['google','cloudflare','amazon','aws','azure','github']
    score = 0
    if any(t in data.get('server','').lower() for t in trusted):
        score -= 15
    if data.get('https_enabled'):
        score -= 5
    return score


def malware_risk_model(data):
    score = 0

    # URL + domain heuristics
    if not data.get('https_enabled'): score += 8
    if data.get('url_length',0) > 75: score += 5
    if data.get('url_shortener'): score += 10
    if data.get('high_risk_tld'): score += 10
    if data.get('suspicious_url_pattern'): score += 8

    # WHOIS
    if not data.get('whois_complete'):
        score += 12
    else:
        try:
            year = int(data['creation_date'][:4])
            age = datetime.now().year - year
            if age <= 1: score += 10
            elif age <= 3: score += 5
        except:
            score += 5

    # JavaScript checks
    if data.get('javascript_obfuscation_ratio',0) > 3: score += 10
    score += min(data.get('js_behavior_score',0), 15)

    # HTML artifact checks
    if data.get('iframe_count',0) > 0: score += 5
    if data.get('external_resources',0) > 10: score += 5
    if data.get('page_size',0) < 1500: score += 4

    # Entropy check
    entropy = url_entropy(data.get('url',''))
    if entropy > 4.5: score += 10

    # Trust adjustment
    score += trust_score_adjustment(data)

    score = max(score, 0)
    risk = min(score, 100)

    confidence = round(100 * (1 - math.exp(-risk / 40)), 2)
    prediction = "Malicious" if confidence >= 60 else "Benign"

    return {
        "prediction": prediction,
        "malware_score": risk,
        "confidence": confidence
    }
